# ğŸ† Practical Performance Enhancement of CNN Core Execution Using Pthread, OpenMP, and CUDA  

## ğŸ“Œ Introduction  

Based on the core convolution calculations of the commonly used **Convolutional Neural Network (CNN)** in artificial intelligence, this project explores **performance enhancement (speedup) using multi-threading** architectures such as **Pthread, OpenMP, and CUDA**.  

ğŸ”¹ **Pthread** and **OpenMP** are executed on a personal computer.  
ğŸ”¹ **CUDA** is executed on **Google Colab** to analyze performance in large-scale multi-threaded environments.  

## ğŸ›  1. Preprocessing  

With rapid advancements in **electronic technology**, computing power has significantly improved. Many **AI models that struggled due to computational limitations** have been greatly enhanced, enabling rapid AI development.  

### ğŸ” **Why CNN?**  
CNN is a widely used AI architecture, where **convolution operations form its core**. The convolution computation follows the formula shown in **Figure 1**, which serves as the basis for **evaluating processing efficiency** and **multi-threaded performance enhancements**.  

### ğŸ¨ **Filters Used**  
Different filters serve different image processing needs. In this study, we apply:  

âœ… **Sobel Filter** â†’ Detects **edges** (Figure 3)  
âœ… **Gaussian Filter** â†’ Performs **blurring** (Figure 4)  

Since convolution requires **extensive matrix computations**, substantial computational power is needed. **Multi-threading** allows multiple tasks to be executed in parallel, significantly increasing speed. More **threads** â†’ **less execution time** â†’ **higher speedup** ğŸš€.  

## âš™ï¸ 2. Implementation with Pthread  

The first implementation uses **Pthread**. It employs:  

ğŸ”¹ `pthread_create` â†’ To create multiple threads  
ğŸ”¹ `pthread_join` â†’ To merge the execution results  

### âš ï¸ **Challenges Faced**  
Initially, we assumed that simply adding these functions would enable multi-threaded execution. However, **incorrect results** appeared. Further debugging revealed issues with:  

âœ… **Upper and lower bounds** â†’ Threads didnâ€™t evenly divide pixel counts  
âœ… **Memory allocation adjustments**  

After fixing these, **Pthread implementation ran correctly** ğŸ‰.  

### ğŸ“Š **Performance Evaluation**  
The program processed images of **various resolutions (320, 1280, 2880, 6000, 11400)** using **Sobel and Gaussian filters**. **Speedup (efficiency gain)** was calculated as the ratio of **single-threaded to multi-threaded execution time** (**Table 1**).  

ğŸ”¹ **More threads generally improve performance** but not always **linearly** ğŸ“ˆ  
ğŸ”¹ **Gaussian filtering** achieves **higher speedup** than **Sobel filtering**  
ğŸ”¹ **Some cases showed speedup exceeding thread count** â†’ due to **overhead reduction**  

## ğŸš€ 3. Implementation with OpenMP  

Next, we switch to **OpenMP**, which is **simpler** and **automatically handles boundary conditions** compared to Pthread.  

### âœ… **Why OpenMP?**  
ğŸ”¹ **Easy syntax**  
ğŸ”¹ **No need to manually handle thread synchronization**  
ğŸ”¹ **Ideal for parallel processing**  

A snippet of **core OpenMP implementation** is shown in **Figure 6**.  

### ğŸ“Š **Performance Results**  
The same experiment was conducted using **OpenMP**, and **Table 2** summarizes **speedup results across different thread counts**. Overall, OpenMP achieved **excellent performance gains** ğŸ¯.  

## âš¡ 4. CUDA and Google Colab Implementation  

To further analyze performance gains, **CUDA implementation** was executed on **Google Colab**.  

### ğŸ”¥ **GPU Used: NVIDIA T4** (Figure 7)  
âœ… **2560 CUDA cores** â†’ Much more powerful than Jetson Nano  
âœ… **Large-scale thread parallelism**  

### âš ï¸ **Challenges with CUDA**  
Unlike **Pthread and OpenMP**, CUDA requires:  
ğŸ”¹ **Explicit memory allocation**  
ğŸ”¹ **Data copying between CPU and GPU**  
ğŸ”¹ **Memory deallocation**  

A portion of the **core CUDA implementation** is shown in **Figure 8**.  

### ğŸ“Š **Speedup Results**  
We tested CUDA using thread counts: **128, 256, 512, and 1024** and compared execution time between **CPU and GPU**. **Table 3** summarizes the results:  

ğŸ”¥ **Sobel Filtering** â†’ Speedup **150Ã— to 504Ã—**  
ğŸ”¥ **Gaussian Filtering** â†’ Speedup **198Ã— to 917Ã—**  

ğŸ”¹ **Figure 9** shows a clear **trend of increasing performance** as **thread count increases** ğŸ“ˆğŸš€.  

## ğŸ¯ Conclusion  

After extensive effort, the **multi-threaded CNN core implementation was successfully developed**. While not fully optimized, this project effectively demonstrates **parallel computing performance enhancements**.  

ğŸ¯ **Future Applications**  
With this strong foundation, we can further explore:  
âœ… **Private blockchain implementation**  
âœ… **Smart contract deployment**  
âœ… **Other AI-related parallel computing applications**  

ğŸ’¡ **This project was both challenging and rewarding!** ğŸš€  
